{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research review: Silver et al., __[Mastering the game of Go without human knowledge](https://www.nature.com/articles/nature24270)__ (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of its large search space, the game of Go is one of the most complicated board games. In 2016 DeepMind's AlphaGo program won a match against Lee Sedol, one the world's strongest players. AlphaGo was described in __[Mastering the game of Go with deep neural networks and tree search](https://www.nature.com/articles/nature16961)__. The paper that we are going to review is about a new version of AlphaGo called AlphaGo Zero. It is called Zero because it uses no knowledge about Go except the rules of the game.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of AlphaGo Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-play\n",
    "The neural network is interpreted as a function $f_\\theta$ that accepts a game position $a$ as input and returns a tuple\n",
    "\n",
    "$$f_\\theta(a) = (\\mathbf{p}, \\mathit{v})$$\n",
    "\n",
    "$\\mathbf{p}$ is a vector of move probabilities. For every legal move from the given game position $a$, $\\mathbf{p}$ contains the probability that this move will be played. $\\mathit{v}$ estimates the probability that the current player will win.\n",
    "\n",
    "A simple approach for self-play would be to just choose a move according to the move probabilities in $\\mathbf{p}$. But stronger play results if the decision is not based on $\\mathbf{p}$ directly, but on a Monte Carlo tree search (MCTS) that uses $(\\mathbf{p}, \\mathit{v})$.\n",
    "\n",
    "This version of MCTS works as follows: first a node in the search tree is selected (looking for a balance between exploration and exploitation). Then this node is expanded, i.e. one of the moves is chosen according to the move probabilities $\\mathbf{p}$ for that position. But unlike in standard versions of MCTS, there is no simulation (playout) for the new node. Instead the value $\\mathit{v}$ provided by the neural network is used directly to evaluate the new position. Then the result is back-propagated up to the root position. After 1.600 expansions the most promising move is played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the neural network\n",
    "From the self-play games, move probabilities are obtained during the MCTS for every position, and the final game result is what $\\mathit{v}$ tries to estimate. So we can compare the values for $f_\\theta(a) = (\\mathbf{p}, \\mathit{v})$ from the neural network with the \"real\" values from the games, and the parameters $\\theta$ of the network can be updated so that the difference between $f_\\theta$ and the games played gets smaller. This yields a new, improved version of $f_\\theta$ that can be used for the next iteration of self-play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences to the previous AlphaGo version\n",
    "-  The most important difference is that AlphaGo Zero uses only reinforcement learning, no supervised learning.\n",
    "-  AlphaGo Zero uses one neural network that computes both the move probabilities and the position evaluations. The previous version used two networks:  a policy network and a value network.\n",
    "-  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aind]",
   "language": "python",
   "name": "conda-env-aind-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
